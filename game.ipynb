{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# N = 3\n",
    "# def get_coupling_map():\n",
    "#     \"\"\"\n",
    "#     Generates a set of possible column pairs.\n",
    "#     Here, we use all adjacent column pairs as an example.\n",
    "#     \"\"\"\n",
    "#     coupling_map = set()\n",
    "#     for i in range(N - 1):\n",
    "#         coupling_map.add((i, i + 1))\n",
    "#     return coupling_map\n",
    "\n",
    "# def get_coupling_map_mat(coupling_map):\n",
    "#     \"\"\"\n",
    "#     Generates an upper triangular matrix from coupling_map.\n",
    "#     For each column pair (i, j) in coupling_map, sets the position (i, j) in the matrix to 1.\n",
    "#     \"\"\"\n",
    "#     coupling_map_mat = np.zeros((N, N))\n",
    "#     for (i, j) in coupling_map:\n",
    "#         coupling_map_mat[i, j] = 1\n",
    "#         coupling_map_mat[j,i] = 1\n",
    "#     return coupling_map_mat  # Make it an upper triangular matrix\n",
    "\n",
    "# def get_initial_state():\n",
    "#     \"\"\"\n",
    "#     Generates an N x N upper triangular matrix consisting of random 0s and 1s.\n",
    "#     \"\"\"\n",
    "#     def _generate_binary_symmetric_matrix():\n",
    "#         upper_triangle = np.triu(np.random.randint(0, 2, size=(N, N)), k=1)\n",
    "#         symmetric_matrix = upper_triangle + upper_triangle.T\n",
    "#         np.fill_diagonal(symmetric_matrix, 0)\n",
    "#         return symmetric_matrix\n",
    "#     mat = _generate_binary_symmetric_matrix()\n",
    "#     mat = mat - np.multiply(mat, coupling_map_mat)\n",
    "#     return mat\n",
    "\n",
    "# coupling_map_mat = get_coupling_map_mat(get_coupling_map())\n",
    "# mat = get_initial_state()\n",
    "# mat = mat-coupling_map_mat*mat\n",
    "# col1, col2 = 1,2\n",
    "# print(mat)\n",
    "# print(col1,col2)\n",
    "# print(coupling_map_mat)\n",
    "# new_mat = mat.copy()\n",
    "# # Swap columns\n",
    "# new_mat[:, [col1, col2]] = new_mat[:, [col2, col1]]\n",
    "# new_mat[[col1, col2],:] = new_mat[[col2, col1],:]\n",
    "# new_mat = new_mat - np.multiply(new_mat, coupling_map_mat)\n",
    "# new_mat = np.clip(new_mat, 0, 1)  # Prevent elements from becoming negative\n",
    "# new_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from rl.network import ResNet\n",
    "from rl.mcts import MCTS\n",
    "from rl.buffer import ReplayBuffer\n",
    "from rl import game\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    state: np.ndarray\n",
    "    mcts_policy: np.ndarray\n",
    "    reward: float\n",
    "\n",
    "\n",
    "def selfplay(weights, test=False):\n",
    "    \"\"\"Perform a self-play game and collect training data.\"\"\"\n",
    "    record = []\n",
    "    if test:\n",
    "        state = game.get_initial_test_state()\n",
    "    else:\n",
    "        state = game.get_initial_state()\n",
    "    game.reset_used_columns_set()\n",
    "    network = ResNet(action_space=game.ACTION_SPACE)\n",
    "\n",
    "    # Initialize network parameters\n",
    "    network.predict(game.encode_state(state))\n",
    "    network.set_weights(weights)\n",
    "\n",
    "    mcts = MCTS(network=network)\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        mcts_policy = mcts.search(\n",
    "            root_state=state, num_simulations=mcts_settings[\"num_mcts_simulations\"]\n",
    "        )\n",
    "        action = np.random.choice(range(game.ACTION_SPACE), p=mcts_policy)\n",
    "        record.append(Sample(state.copy(), mcts_policy, reward=None))\n",
    "        state, action_score, done = game.step(state, action, mcts_policy)\n",
    "        print(state, action_score, done)\n",
    "        total_score += action_score\n",
    "        step_count += 1\n",
    "    print(\"======================\")\n",
    "\n",
    "    # The reward is calculated based on the final state\n",
    "    reward = game.get_reward(state, total_score)\n",
    "\n",
    "    # Assign the reward to each sample\n",
    "    for sample in record:\n",
    "        sample.reward = reward\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 1 True\n",
      "======================\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 1 False\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 False\n",
      "======================\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "======================\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 True\n",
      "======================\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 True\n",
      "======================\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 1 False\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 False\n",
      "======================\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "======================\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 True\n",
      "======================\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 True\n",
      "======================\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] 4 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]] 1 False\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] 4 True\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "\n",
    "# ray.init(num_cpus=num_cpus, num_gpus=1, local_mode=False)\n",
    "\n",
    "logdir = Path(\"log\")\n",
    "if logdir.exists():\n",
    "    shutil.rmtree(logdir)\n",
    "summary_writer = tf.summary.create_file_writer(str(logdir))\n",
    "\n",
    "game.initialize_game()  # Initialize game variables\n",
    "\n",
    "network = ResNet(action_space=game.ACTION_SPACE)\n",
    "\n",
    "dummy_state = game.encode_state(game.get_initial_state())\n",
    "network.predict(dummy_state)\n",
    "\n",
    "current_weights = network.get_weights()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=network_settings[\"learning_rate\"])\n",
    "\n",
    "replay = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "# # Start self-play workers\n",
    "work_in_progresses = [selfplay(current_weights, True)]\n",
    "\n",
    "\n",
    "n_updates = 0\n",
    "n = 0\n",
    "while n <= n_episodes:\n",
    "    for _ in range(update_period):\n",
    "        # Wait for a self-play worker to finish\n",
    "        finished = selfplay(current_weights, True)\n",
    "        replay.add_record(finished)\n",
    "        n += 1\n",
    "\n",
    "    # Update network\n",
    "    if len(replay) >= batch_size:\n",
    "        num_iters = epochs_per_update * (len(replay) // batch_size)\n",
    "        for i in range(num_iters):\n",
    "            states, mcts_policy, rewards = replay.get_minibatch(batch_size=batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                p_pred, v_pred = network(states, training=True)\n",
    "                value_loss = tf.square(rewards - v_pred)\n",
    "                policy_loss = -tf.reduce_sum(\n",
    "                    mcts_policy * tf.math.log(p_pred + 1e-10), axis=1, keepdims=True\n",
    "                )\n",
    "                loss = tf.reduce_mean(value_loss + policy_loss)\n",
    "\n",
    "            grads = tape.gradient(loss, network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "            n_updates += 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\n",
    "                        \"value_loss\", tf.reduce_mean(value_loss), step=n_updates\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        \"policy_loss\", tf.reduce_mean(policy_loss), step=n_updates\n",
    "                    )\n",
    "\n",
    "        current_weights = network.get_weights()\n",
    "\n",
    "    if n % save_period == 0:\n",
    "        network.save_weights(\"checkpoints/network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_transpiler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
