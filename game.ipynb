{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import ray\n",
    "\n",
    "from rl.network import ResNet\n",
    "from rl.mcts import MCTS\n",
    "from rl.buffer import ReplayBuffer\n",
    "\n",
    "from rl.game import Game, encode_state\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "index = \"20241120\"\n",
    "qubits = config[\"game_settings\"][\"N\"]\n",
    "a = config[\"game_settings\"][\"a\"]\n",
    "b = config[\"game_settings\"][\"b\"]\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "num_gpus = training_settings[\"num_gpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "game = Game(qubits, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    state: np.ndarray\n",
    "    mcts_policy: np.ndarray\n",
    "    reward: float\n",
    "\n",
    "\n",
    "@ray.remote(num_cpus=num_cpus, num_gpus=num_gpus)\n",
    "def selfplay(weights, qubits, current_episode, test=False):\n",
    "    \"\"\"Perform a self-play game and collect training data.\"\"\"\n",
    "    record = []\n",
    "    if test:\n",
    "        state = game.get_initial_test_state()\n",
    "    else:\n",
    "        state = game.state\n",
    "    game.reset_used_columns()\n",
    "    network = ResNet(action_space=len(game.coupling_map))\n",
    "\n",
    "    # Initialize network parameters\n",
    "    network.predict(encode_state(state, qubits))\n",
    "    network.set_weights(weights)\n",
    "\n",
    "    mcts = MCTS(qubits=qubits, network=network)\n",
    "    mcts.current_episode = current_episode\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        mcts_policy = mcts.search(\n",
    "            root_state=state,\n",
    "            num_simulations=mcts_settings[\"num_mcts_simulations\"],\n",
    "            prev_action=prev_action,\n",
    "        )\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(len(game.coupling_map)) if i != prev_action]\n",
    "            prob = mcts_policy[indices]\n",
    "            prob = prob / prob.sum()\n",
    "            # if use_network_policy:\n",
    "            #     prob = np.ones(len(prob))/len(prob)\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        else:\n",
    "            indices = list(range(len(game.coupling_map)))\n",
    "            prob = mcts_policy\n",
    "            # if use_network_policy:\n",
    "            #     prob = np.ones(len(mcts_policy))/len(mcts_policy)\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        record.append(Sample(state.copy(), mcts_policy, reward=None))\n",
    "        state, done, action_score = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        # print(state, action_score, done)\n",
    "        total_score += action_score\n",
    "        step_count += 1\n",
    "\n",
    "    # The reward is calculated based on the final state\n",
    "    reward = game.get_reward(state, total_score)\n",
    "    # Assign the reward to each sample\n",
    "    for sample in record:\n",
    "        sample.reward = reward\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 11:40:03,294\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 8.0, 'memory': 41083504231.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 2147483648.0, 'GPU': 8.0, 'node:__internal_head__': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [5:34:22<00:00, 401.24s/it]  \n",
      "100%|██████████| 50/50 [4:51:03<00:00, 349.27s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [4:33:57<00:00, 328.75s/it]  \n",
      "100%|██████████| 50/50 [4:45:24<00:00, 342.49s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [4:34:56<00:00, 329.93s/it]  \n",
      "100%|██████████| 50/50 [4:38:47<00:00, 334.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=num_cpus, num_gpus=num_gpus, local_mode=False, include_dashboard=True)\n",
    "print(ray.available_resources())\n",
    "logdir = Path(\"log\")\n",
    "if logdir.exists():\n",
    "    shutil.rmtree(logdir)\n",
    "summary_writer = tf.summary.create_file_writer(str(logdir))\n",
    "\n",
    "\n",
    "network = ResNet(action_space=len(game.coupling_map))\n",
    "\n",
    "dummy_state = encode_state(game.state, qubits)\n",
    "network.predict(dummy_state)\n",
    "\n",
    "# current_weights = network.get_weights()\n",
    "current_weights = ray.put(network.get_weights())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=network_settings[\"learning_rate\"])\n",
    "\n",
    "replay = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "# # Start self-play workers\n",
    "# work_in_progresses = [selfplay(current_weights,qubits, True)]\n",
    "work_in_progresses = [\n",
    "    selfplay.remote(current_weights, qubits, True) for _ in range(num_cpus - 1)\n",
    "]\n",
    "n_updates = 0\n",
    "\n",
    "n = 0\n",
    "while n < n_episodes:\n",
    "    for _ in tqdm(range(update_period)):\n",
    "        # finished = selfplay(current_weights,qubits, n, test=False)\n",
    "        # replay.add_record(finished)\n",
    "        finished, work_in_progresses = ray.wait(work_in_progresses, num_returns=1)\n",
    "        replay.add_record(ray.get(finished[0]))\n",
    "        work_in_progresses.extend([selfplay.remote(current_weights, qubits, True)])\n",
    "        n += 1\n",
    "\n",
    "    # Update network\n",
    "    if len(replay) >= batch_size:\n",
    "        num_iters = epochs_per_update * (len(replay) // batch_size)\n",
    "        value_loss_weight = 1\n",
    "        policy_loss_weight = 1\n",
    "        for i in range(num_iters):\n",
    "            states, mcts_policy, rewards = replay.get_minibatch(batch_size=batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                p_pred, v_pred = network(states, training=True)\n",
    "                value_loss = tf.square(rewards - v_pred)\n",
    "                policy_loss = -tf.reduce_sum(\n",
    "                    mcts_policy * tf.math.log(p_pred + 1e-5), axis=1, keepdims=True\n",
    "                )\n",
    "                loss = tf.reduce_mean(\n",
    "                    value_loss_weight * value_loss + policy_loss_weight * policy_loss\n",
    "                )\n",
    "            grads = tape.gradient(loss, network.trainable_variables)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "            n_updates += 1\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\n",
    "                        \"value_loss\", tf.reduce_mean(value_loss), step=n_updates\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        \"policy_loss\", tf.reduce_mean(policy_loss), step=n_updates\n",
    "                    )\n",
    "\n",
    "        # current_weights = network.get_weights()\n",
    "        current_weights = ray.put(network.get_weights())\n",
    "\n",
    "    if n % save_period == 0:\n",
    "        network.save(f\"checkpoints/network{qubits}_{index}_{n}.keras\")\n",
    "        network.save_weights(f\"checkpoints/network{qubits}_{index}_{n}.weights.h5\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rl.network import ResNet\n",
    "import rl.game as game\n",
    "\n",
    "game = Game(qubits, config)\n",
    "\n",
    "network = ResNet(action_space=len(game.coupling_map))\n",
    "# network = tf.keras.models.load_model(f\"checkpoints/network{qubits}_{index}_700\")\n",
    "# 初期状態の生成\n",
    "network.load_weights(f\"checkpoints/network{qubits}_{index}_700.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(12):\n",
    "    state = game.state\n",
    "    ans = []\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "    print(state)\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        # 状態のエンコードと次元の調整\n",
    "        encoded_state = encode_state(state, qubits)\n",
    "        input_state = np.expand_dims(encoded_state, axis=0)\n",
    "\n",
    "        # モデルによる予測\n",
    "        policy_output, value_output = network.predict(input_state)\n",
    "        policy = policy_output[0]\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(len(game.coupling_map)) if i != prev_action]\n",
    "\n",
    "            prob = policy[indices]\n",
    "            action = np.random.choice(indices, p=prob / prob.sum())\n",
    "        else:\n",
    "            indices = list(range(len(game.coupling_map)))\n",
    "            action = np.random.choice(indices, p=policy)\n",
    "        selected_action = game.coupling_map[action]\n",
    "        # print(f\"Step {step_count}: Selected action {selected_action}\")\n",
    "        ans.append(selected_action)\n",
    "        # アクションの適用\n",
    "        state, done, _ = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        step_count += 1\n",
    "\n",
    "        # 現在の状態を表示（必要に応じて）\n",
    "        # print(\"Current state:\\n\", state)\n",
    "        # print(\"Game done:\", done)\n",
    "        # print(\"-\" * 50)\n",
    "\n",
    "    # ゲーム結果の表示\n",
    "    if done:\n",
    "        print(f\"Game finished successfully in {step_count} steps with {ans}\")\n",
    "    else:\n",
    "        print(f\"Game terminated after reaching the maximum steps ({game.MAX_STEPS}).\")\n",
    "        print(f\"Total score: {total_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitranspiler",
   "language": "python",
   "name": "aitranspiler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
