{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from rl.network import ResNet\n",
    "from rl.mcts import MCTS\n",
    "from rl.buffer import ReplayBuffer, Sample\n",
    "from rl.game import Game, encode_state\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "base_path = \"graphs\"\n",
    "index = \"20241130\"\n",
    "qubits = config[\"game_settings\"][\"N\"]\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "num_gpus = training_settings[\"num_gpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "eval_period = training_settings.get(\"eval_period\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "def selfplay(weights, qubits, current_episode, config):\n",
    "    record = []\n",
    "    game = Game(qubits, config)\n",
    "    state = game.get_initial_state()\n",
    "    game.reset_used_columns()\n",
    "    network = ResNet(action_space=len(game.coupling_map), config=config)\n",
    "    network.predict(encode_state(state, qubits))\n",
    "    network.set_weights(weights)\n",
    "\n",
    "    mcts = MCTS(qubits=qubits, network=network, config=config)\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        mcts_policy = mcts.search(\n",
    "            root_state=state,\n",
    "            prev_action=prev_action,\n",
    "            num_simulations=mcts_settings[\"num_mcts_simulations\"],\n",
    "        )\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(len(game.coupling_map)) if i != prev_action]\n",
    "            prob = mcts_policy[indices]\n",
    "            prob = prob/ prob.sum()\n",
    "            # if use_network_policy:\n",
    "            #     prob = np.ones(len(prob))/len(prob)\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        else:\n",
    "            indices = list(range(len(game.coupling_map)))\n",
    "            prob = mcts_policy\n",
    "            # if use_network_policy:\n",
    "            #     prob = np.ones(len(mcts_policy))/len(mcts_policy)\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        record.append(Sample(state.copy(), mcts_policy, reward=None))\n",
    "        state, done, action_score = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        # print(state, action_score, done)\n",
    "        total_score += action_score\n",
    "        step_count += 1\n",
    "\n",
    "    reward = game.get_reward(state, total_score)\n",
    "    for sample in record:\n",
    "        sample.reward = reward\n",
    "    return record\n",
    "\n",
    "def evaluate_self_play(qubits, network, config):\n",
    "    pattern = os.path.join(base_path, f\"adj_matrix_{qubits}_*.npy\")\n",
    "    file_paths = glob.glob(pattern)\n",
    "    avg_depth = []\n",
    "    avg_counts = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        state = np.load(file_path)\n",
    "        game = Game(qubits, config)\n",
    "        swap_pairs = []\n",
    "        done = False\n",
    "        total_score = 0\n",
    "        step_count = 0\n",
    "        prev_action = None\n",
    "        while not done and step_count < game.MAX_STEPS:\n",
    "            encoded_state = encode_state(state, qubits)\n",
    "            input_state = np.expand_dims(encoded_state, axis=0)\n",
    "            policy_logits, value_output = network.predict(input_state)\n",
    "            policy = tf.nn.softmax(policy_logits).numpy()[0]\n",
    "            valid_actions = game.get_valid_actions(state, prev_action)\n",
    "            policy = np.array([policy[a] if a in valid_actions else 0 for a in range(len(game.coupling_map))])\n",
    "            policy_sum = np.sum(policy)\n",
    "            if policy_sum > 0:\n",
    "                policy /= policy_sum\n",
    "            else:\n",
    "                policy[valid_actions] = 1 / len(valid_actions)\n",
    "            action = np.random.choice(len(policy), p=policy)\n",
    "            selected_action = game.coupling_map[action]\n",
    "            swap_pairs.append(selected_action)\n",
    "            state, done, _ = game.step(state, action, prev_action)\n",
    "            prev_action = action\n",
    "            step_count += 1\n",
    "        if not done:\n",
    "            depth = game.MAX_STEPS\n",
    "            swap_count = game.MAX_STEPS\n",
    "        else:\n",
    "            depth = game.current_layer\n",
    "            swap_count = len(swap_pairs)\n",
    "        avg_counts.append(swap_count)\n",
    "        avg_depth.append(depth)\n",
    "    return np.mean(avg_depth), np.mean(avg_counts)\n",
    "\n",
    "logdir = Path(\"log\")\n",
    "if logdir.exists():\n",
    "    shutil.rmtree(logdir)\n",
    "summary_writer = tf.summary.create_file_writer(str(logdir))\n",
    "\n",
    "game = Game(qubits, config)\n",
    "network = ResNet(action_space=len(game.coupling_map), config=config)\n",
    "\n",
    "dummy_state = encode_state(game.state, qubits)\n",
    "network.predict(encode_state(game.state, qubits))\n",
    "current_weights = network.get_weights()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=network_settings[\"learning_rate\"])\n",
    "\n",
    "replay = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "n_updates = 0\n",
    "n = 0\n",
    "\n",
    "while n < n_episodes:\n",
    "    for _ in tqdm(range(update_period)):\n",
    "        finished = selfplay(current_weights,qubits,n, config)\n",
    "        replay.add_record(finished)\n",
    "        n += 1\n",
    "\n",
    "    if len(replay) >= batch_size:\n",
    "        num_iters = epochs_per_update * (len(replay) // batch_size)\n",
    "        value_loss_weight = 1.0\n",
    "        policy_loss_weight = 1.0\n",
    "        for i in tqdm(range(num_iters)):\n",
    "            states, mcts_policy, rewards = replay.get_minibatch(batch_size=batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                p_pred, v_pred = network(states, training=True)\n",
    "                value_loss = tf.square(rewards - v_pred)\n",
    "                policy_loss = -tf.reduce_sum(\n",
    "                    mcts_policy * tf.math.log(p_pred + 1e-5), axis=1, keepdims=True\n",
    "                )\n",
    "                loss = tf.reduce_mean(value_loss_weight * value_loss + policy_loss_weight * policy_loss)\n",
    "            grads = tape.gradient(loss, network.trainable_variables)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "            n_updates += 1\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\n",
    "                        \"value_loss\", tf.reduce_mean(value_loss), step=n_updates\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        \"policy_loss\", tf.reduce_mean(policy_loss), step=n_updates\n",
    "                    )\n",
    "\n",
    "        current_weights = network.get_weights()\n",
    "\n",
    "    # モデルの保存と評価\n",
    "    if n % save_period == 0:\n",
    "        network.save(f\"checkpoints/network{qubits}_{index}_{n}.keras\")\n",
    "        network.save_weights(f\"checkpoints/network{qubits}_{index}_{n}.weights.h5\")\n",
    "\n",
    "    # if n % eval_period == 0:\n",
    "    #     depth, count = evaluate_self_play(qubits, network, config)\n",
    "    #     print(f\"Episode {n}: SWAP depth is {depth}, SWAP count is {count}\")\n",
    "    #     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m network \u001b[38;5;241m=\u001b[39m ResNet(action_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(game\u001b[38;5;241m.\u001b[39mcoupling_map),config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# network = tf.keras.models.load_model(f\"checkpoints/network{qubits}_{index}_10.keras\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 初期状態の生成\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# network.load_weights(f\"checkpoints/network{qubits}_{index}_700.weights.h5\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# network.save(f\"checkpoints/network{qubits}_{index}_700.keras\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/network\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mqubits\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mindex\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_700.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Develop/venv/ai_transpiler/lib/python3.12/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/Develop/venv/ai_transpiler/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:365\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Develop/venv/ai_transpiler/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:442\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    440\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 442\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    447\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Develop/venv/ai_transpiler/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:431\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 431\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Develop/venv/ai_transpiler/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/Develop/AITranspiler/rl/network.py:104\u001b[0m, in \u001b[0;36mResNet.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Extract `network_settings` and pass it as the `config` argument\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     action_space \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    105\u001b[0m     network_settings \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(action_space, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: network_settings})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'action_space'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rl.network import ResNet\n",
    "import rl.game as game\n",
    "\n",
    "game = Game(qubits, config)\n",
    "\n",
    "network = ResNet(action_space=len(game.coupling_map),config=config)\n",
    "network = tf.keras.models.load_model(f\"checkpoints/network{qubits}_{index}_700.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 1. 0. 0.]]\n",
      "Game terminated after reaching the maximum steps (25).\n",
      "Total score: 0\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Game finished successfully in 4 steps with [(4, 5), (1, 2), (4, 5), (2, 3)]\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "Game finished successfully in 13 steps with [(4, 5), (0, 1), (4, 5), (2, 3), (4, 5), (0, 1), (4, 5), (2, 3), (4, 5), (3, 4), (4, 5), (2, 3), (4, 5)]\n",
      "[[0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]]\n",
      "Game finished successfully in 18 steps with [(4, 5), (2, 3), (4, 5), (3, 4), (4, 5), (2, 3), (4, 5), (3, 4), (4, 5), (0, 1), (4, 5), (1, 2), (4, 5), (2, 3), (4, 5), (1, 2), (4, 5), (3, 4)]\n",
      "[[0. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]]\n",
      "Game finished successfully in 12 steps with [(4, 5), (1, 2), (4, 5), (2, 3), (4, 5), (1, 2), (4, 5), (3, 4), (4, 5), (1, 2), (4, 5), (3, 4)]\n",
      "[[0. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]]\n",
      "Game finished successfully in 19 steps with [(4, 5), (2, 3), (4, 5), (0, 1), (4, 5), (1, 2), (4, 5), (0, 1), (4, 5), (3, 4), (4, 5), (3, 4), (4, 5), (1, 2), (4, 5), (2, 3), (4, 5), (2, 3), (4, 5)]\n",
      "[[0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "Game terminated after reaching the maximum steps (25).\n",
      "Total score: 0\n",
      "[[0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0.]]\n",
      "Game finished successfully in 23 steps with [(4, 5), (3, 4), (4, 5), (1, 2), (4, 5), (0, 1), (4, 5), (3, 4), (4, 5), (3, 4), (4, 5), (1, 2), (4, 5), (1, 2), (4, 5), (1, 2), (4, 5), (2, 3), (4, 5), (1, 2), (4, 5), (3, 4), (4, 5)]\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0.]]\n",
      "Game finished successfully in 13 steps with [(4, 5), (3, 4), (4, 5), (1, 2), (4, 5), (1, 2), (4, 5), (3, 4), (4, 5), (1, 2), (4, 5), (2, 3), (4, 5)]\n",
      "[[0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]]\n",
      "Game terminated after reaching the maximum steps (25).\n",
      "Total score: 0\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]]\n",
      "Game finished successfully in 23 steps with [(4, 5), (1, 2), (4, 5), (3, 4), (4, 5), (0, 1), (4, 5), (1, 2), (4, 5), (3, 4), (4, 5), (0, 1), (4, 5), (1, 2), (4, 5), (2, 3), (4, 5), (2, 3), (4, 5), (1, 2), (4, 5), (2, 3), (4, 5)]\n",
      "[[0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]]\n",
      "Game finished successfully in 8 steps with [(4, 5), (0, 1), (4, 5), (2, 3), (4, 5), (1, 2), (4, 5), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(12):\n",
    "    game = Game(qubits, config)\n",
    "\n",
    "    state = game.state\n",
    "    ans = []\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "    print(state)\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        encoded_state = encode_state(state, qubits)\n",
    "        input_state = np.expand_dims(encoded_state, axis=0)\n",
    "\n",
    "        policy_output, value_output = network.predict(input_state)\n",
    "        policy = policy_output[0]\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(len(game.coupling_map)) if i != prev_action]\n",
    "\n",
    "            prob = policy.numpy()[indices]\n",
    "            if prob.sum() < 1e-6:\n",
    "                action = np.random.choice(indices)\n",
    "            else:\n",
    "                action = np.random.choice(indices, p=prob / prob.sum())\n",
    "        else:\n",
    "            indices = list(range(len(game.coupling_map)))\n",
    "            action = np.random.choice(indices, p=policy)\n",
    "        selected_action = game.coupling_map[action]\n",
    "        ans.append(selected_action)\n",
    "        state, done, _ = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        step_count += 1\n",
    "\n",
    "    if done:\n",
    "        print(f\"Game finished successfully in {step_count} steps with {ans}\")\n",
    "    else:\n",
    "        print(f\"Game terminated after reaching the maximum steps ({game.MAX_STEPS}).\")\n",
    "        print(f\"Total score: {total_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.633333333333333, 18.066666666666666)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_self_play(qubits,network,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitranspiler",
   "language": "python",
   "name": "aitranspiler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
